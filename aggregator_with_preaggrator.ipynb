{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreAgg: NNM, Aggregator: trimmed_mean, Byzantine ratio: 0%, Accuracy: 97.59%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 416\u001b[39m\n\u001b[32m    414\u001b[39m results = []\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ratio \u001b[38;5;129;01min\u001b[39;00m byzantine_ratios:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     acc = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mtotal_time_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_time_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mbyzantine_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m                         \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43magg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mpreagg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreagg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m     results.append(acc)\n\u001b[32m    423\u001b[39m     method_name = preagg \u001b[38;5;28;01mif\u001b[39;00m preagg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNoPreAgg\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 398\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(num_runs, total_time_steps, verbose, byzantine_ratio, aggregation_strategy, preagg)\u001b[39m\n\u001b[32m    396\u001b[39m             workers.append(Worker(server.model, server.device, i, availability, processing_time))\n\u001b[32m    397\u001b[39m     simulator = DiscreteEventSimulator(server, workers, test_loader, total_time_steps=total_time_steps)\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     acc = \u001b[43msimulator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     accuracy_list.append(acc)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(accuracy_list)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 327\u001b[39m, in \u001b[36mDiscreteEventSimulator.run\u001b[39m\u001b[34m(self, verbose)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m worker.has_finished(\u001b[38;5;28mself\u001b[39m.global_time):\n\u001b[32m    326\u001b[39m     mini_idx = worker.current_batch_info[\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     gradients, batch_info = \u001b[43mworker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_mini_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmini_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m     \u001b[38;5;28mself\u001b[39m.server.register_gradient(gradients, batch_info, worker.worker_id,\n\u001b[32m    329\u001b[39m                                   \u001b[38;5;28mself\u001b[39m.global_time, worker.processing_time)\n\u001b[32m    330\u001b[39m     finished_workers.append(worker)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mWorker.compute_gradient\u001b[39m\u001b[34m(self, data, targets)\u001b[39m\n\u001b[32m    164\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(data)\n\u001b[32m    165\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(outputs, targets)\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    167\u001b[39m gradients = {name: param.grad.detach().clone() \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.named_parameters()}\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m gradients, \u001b[38;5;28mself\u001b[39m.current_batch_info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import byzfl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Fusionne les intervalles qui se chevauchent ou sont adjacents.\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "    intervals.sort()\n",
    "    merged = [intervals[0]]\n",
    "    for current in intervals[1:]:\n",
    "        last = merged[-1]\n",
    "        if current[0] <= last[1]:\n",
    "            merged[-1] = (last[0], max(last[1], current[1]))\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    return merged\n",
    "\n",
    "def generate_gaussian_availability(total_time_steps, worker_id, num_workers, std_dev_ratio=0.1, min_processing_time=3):\n",
    "    \"\"\"\n",
    "    Disponibilité honnête : 3 pics durant les heures pleines.\n",
    "    \"\"\"\n",
    "    # Trois pics bien définis (matin, après-midi, soir)\n",
    "    peak_means = [0.25, 0.5, 0.75]  # en proportion du total\n",
    "    std_dev = int(std_dev_ratio * total_time_steps)\n",
    "\n",
    "    availability_periods = []\n",
    "\n",
    "    for mean_ratio in peak_means:\n",
    "        mean = int(mean_ratio * total_time_steps)\n",
    "        intensity = random.choice([1, 2, 3])  # intensité aléatoire\n",
    "        for _ in range(intensity):\n",
    "            start_time = int(np.random.normal(loc=mean, scale=std_dev))\n",
    "            start_time = max(0, min(start_time, total_time_steps - min_processing_time))\n",
    "            max_extra = 2 * intensity\n",
    "            duration = random.randint(min_processing_time, min(min_processing_time + max_extra, total_time_steps - start_time))\n",
    "            end_time = start_time + duration\n",
    "            availability_periods.append((start_time, end_time))\n",
    "\n",
    "    return merge_intervals(availability_periods)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_gaussian_byzantine_availability(total_time_steps, worker_id, num_workers, std_dev_ratio=0.08, min_processing_time=2):\n",
    "    \"\"\"\n",
    "    Disponibilité byzantine : pics dans les heures creuses uniquement.\n",
    "    \"\"\"\n",
    "    # Heures creuses : nuit/tôt le matin ou entre les pics honnêtes\n",
    "    off_peak_means = [0.1, 0.375, 0.9]  # en proportion\n",
    "    std_dev = int(std_dev_ratio * total_time_steps)\n",
    "\n",
    "    availability_periods = []\n",
    "\n",
    "    for mean_ratio in off_peak_means:\n",
    "        mean = int(mean_ratio * total_time_steps)\n",
    "        intensity = random.choice([1, 1, 2])  # faible à moyenne\n",
    "        for _ in range(intensity):\n",
    "            start_time = int(np.random.normal(loc=mean, scale=std_dev))\n",
    "            start_time = max(0, min(start_time, total_time_steps - min_processing_time))\n",
    "            max_extra = 1 * intensity\n",
    "            duration = random.randint(1, min(min_processing_time + max_extra, total_time_steps - start_time))\n",
    "            end_time = start_time + duration\n",
    "            availability_periods.append((start_time, end_time))\n",
    "\n",
    "    return merge_intervals(availability_periods)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Définition du modèle de réseau de neurones\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LeNetLike(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetLike, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)   # 28x28 → 28x28\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)  # 14x14 → 14x14\n",
    "        self.pool = nn.MaxPool2d(2, 2)                # pooling: 28→14→7\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [batch, 32, 14, 14]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # [batch, 64, 7, 7]\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Gestionnaire de batch complet avec SubsetRandomSampler\n",
    "class BatchManager:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = list(range(len(dataset)))\n",
    "        random.shuffle(self.indices)\n",
    "        self.sampler = SubsetRandomSampler(self.indices)\n",
    "        self.loader = iter(DataLoader(self.dataset, batch_size=self.batch_size, sampler=self.sampler))\n",
    "    def get_next_batch(self):\n",
    "        try:\n",
    "            return next(self.loader)\n",
    "        except StopIteration:\n",
    "            random.shuffle(self.indices)\n",
    "            self.sampler = SubsetRandomSampler(self.indices)\n",
    "            self.loader = iter(DataLoader(self.dataset, batch_size=self.batch_size, sampler=self.sampler))\n",
    "            return next(self.loader)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Worker adapté pour traiter un mini-batch identifié par (complete_batch_id, mini_batch_id)\n",
    "class Worker:\n",
    "    def __init__(self, model, device, worker_id, availability_periods, processing_time):\n",
    "        self.device = device\n",
    "        self.worker_id = worker_id\n",
    "        self.availability_periods = availability_periods\n",
    "        self.processing_time = processing_time\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.finish_time = None\n",
    "        self.current_batch_info = None  # (complete_batch_id, mini_batch_id)\n",
    "    def is_available(self, current_time):\n",
    "        for start_time, end_time in self.availability_periods:\n",
    "            if start_time <= current_time < end_time:\n",
    "                if self.finish_time is None or current_time >= self.finish_time:\n",
    "                    return (current_time + self.processing_time) <= end_time\n",
    "        return False\n",
    "    def start_computation(self, current_time, batch_info, data, targets):\n",
    "        if self.current_batch_info is None or self.current_batch_info != batch_info:\n",
    "            self.current_batch_info = batch_info\n",
    "            self.finish_time = current_time + self.processing_time\n",
    "            return self.compute_gradient(data, targets)\n",
    "        return None\n",
    "    def has_finished(self, current_time):\n",
    "        return self.finish_time == current_time\n",
    "    def compute_gradient(self, data, targets):\n",
    "        data, targets = data.to(self.device), targets.to(self.device)\n",
    "        self.model.zero_grad()\n",
    "        outputs = self.model(data)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        gradients = {name: param.grad.detach().clone() for name, param in self.model.named_parameters()}\n",
    "        return gradients, self.current_batch_info\n",
    "    def update_model(self, model_state_dict):\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Worker byzantin qui renvoie un gradient corrompu\n",
    "class ByzantineWorker(Worker):\n",
    "    def compute_gradient(self, data, targets):\n",
    "        corrupted_gradients = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            corrupted_gradients[name] = torch.full_like(param, 1e6)\n",
    "        return corrupted_gradients, self.current_batch_info\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Serveur qui gère le batch complet, le découpage en mini-batches et l'agrégation des gradients.\n",
    "# On ajoute un paramètre \"preagg\" pour choisir la méthode de pré-agrégation.\n",
    "class Server:\n",
    "    def __init__(self, dataset, batch_size=64, mini_batch_size=16, lr=0.005, verbose=False,\n",
    "                 aggregation_strategy='trimmed_mean', f=0, preagg=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = LeNetLike().to(self.device) #CNN().to(self.device)   #SimpleNN().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.batch_manager = BatchManager(dataset, batch_size)\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.current_complete_batch = None\n",
    "        self.current_batch_id = 0\n",
    "        self.current_mini_batches = {}\n",
    "        self.received_gradients = {}\n",
    "        self.verbose = verbose\n",
    "        self.aggregation_strategy = aggregation_strategy\n",
    "        self.f = f\n",
    "        self.preagg = preagg  # Méthode de pré-agrégation (string ou None)\n",
    "\n",
    "    def get_new_complete_batch(self, current_time):\n",
    "        data, targets = self.batch_manager.get_next_batch()\n",
    "        self.current_complete_batch = (data, targets)\n",
    "        self.current_batch_id += 1\n",
    "\n",
    "        num_samples = data.size(0)\n",
    "        mini_batches = []\n",
    "        for i in range(0, num_samples, self.mini_batch_size):\n",
    "            mini_data = data[i: i + self.mini_batch_size]\n",
    "            mini_targets = targets[i: i + self.mini_batch_size]\n",
    "            mini_batches.append((mini_data, mini_targets))\n",
    "        self.current_mini_batches = {i: mini_batches[i] for i in range(len(mini_batches))}\n",
    "        self.received_gradients = {}\n",
    "        if self.verbose:\n",
    "            print(f\"[Temps {current_time}] Batch complet {self.current_batch_id} divisé en {len(mini_batches)} mini-batches.\")\n",
    "\n",
    "    def register_gradient(self, gradients, batch_info, worker_id, current_time, worker_processing_time):\n",
    "        batch_id, mini_idx = batch_info\n",
    "        if batch_id != self.current_batch_id:\n",
    "            return\n",
    "        if mini_idx in self.received_gradients:\n",
    "            if self.verbose:\n",
    "                print(f\"[Temps {current_time}] Doublon pour mini-batch {mini_idx} par worker {worker_id}.\")\n",
    "            return\n",
    "        self.received_gradients[mini_idx] = gradients\n",
    "        if self.verbose:\n",
    "            print(f\"[Temps {current_time}] Gradient accepté pour mini-batch {mini_idx} (batch {batch_id}) par worker {worker_id}.\")\n",
    "\n",
    "    def aggregate_and_update(self):\n",
    "        if len(self.received_gradients) != len(self.current_mini_batches):\n",
    "            if self.verbose:\n",
    "                print(\"Attention : tous les mini-batches n'ont pas été traités.\")\n",
    "            return False\n",
    "\n",
    "        aggregated_gradients = {}\n",
    "        self.f = 4\n",
    "        # Choix de la pré-agrégation\n",
    "        if self.preagg == \"NNM\":\n",
    "            preaggregator = byzfl.NNM(f=self.f)\n",
    "        elif self.preagg == \"Bucketing\":\n",
    "            preaggregator = byzfl.Bucketing(s=1)\n",
    "        elif self.preagg == \"ARC\":\n",
    "            preaggregator = byzfl.ARC(f=self.f)\n",
    "        else:\n",
    "            preaggregator = None\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            grads = [self.received_gradients[idx][name] for idx in self.received_gradients]\n",
    "            grads_tensor = torch.stack(grads)  # shape: (n, *)\n",
    "            original_shape = grads_tensor.shape[1:]\n",
    "            grads_tensor_flat = grads_tensor.view(grads_tensor.size(0), -1)  # shape: (n, d)\n",
    "\n",
    "            n = grads_tensor_flat.shape[0]\n",
    "            effective_f = max(self.f,  (n // 2) - 1)\n",
    "\n",
    "            # Pré-agrégation\n",
    "            if preaggregator is not None:\n",
    "                try:\n",
    "                    preagg_flat = preaggregator(grads_tensor_flat)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur de pré-agrégation ({self.preagg}): {e}\")\n",
    "                    preagg_flat = grads_tensor_flat\n",
    "            else:\n",
    "                preagg_flat = grads_tensor_flat\n",
    "\n",
    "            # Agrégation principale\n",
    "            if self.aggregation_strategy == \"trimmed_mean\":\n",
    "                aggregator = byzfl.TrMean(f=effective_f)\n",
    "            elif self.aggregation_strategy == \"multi_krum\":\n",
    "                aggregator = byzfl.MultiKrum(f=effective_f)\n",
    "            elif self.aggregation_strategy == \"geometric_median\":\n",
    "                aggregator = byzfl.GeometricMedian()\n",
    "            elif self.aggregation_strategy == \"average\":\n",
    "                aggregator = byzfl.Average()\n",
    "            elif self.aggregation_strategy == \"meamed\":\n",
    "                aggregator = byzfl.Meamed(f=effective_f)\n",
    "            else:\n",
    "                raise ValueError(f\"Stratégie d’agrégation inconnue : {self.aggregation_strategy}\")\n",
    "\n",
    "            agg_flat = aggregator(preagg_flat)  # shape: (d,)\n",
    "            aggregated_gradients[name] = agg_flat.view(original_shape)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.grad = aggregated_gradients[name]\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Modèle mis à jour avec le batch complet {self.current_batch_id}.\")\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in test_loader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        return 100 * correct / total\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Simulation d'apprentissage fédéré en mode événement discret\n",
    "class DiscreteEventSimulator:\n",
    "    def __init__(self, server, workers, test_loader, total_time_steps=20):\n",
    "        self.server = server\n",
    "        self.workers = workers\n",
    "        self.test_loader = test_loader\n",
    "        self.total_time_steps = total_time_steps\n",
    "        self.global_time = 0\n",
    "        self.in_progress_workers = []\n",
    "\n",
    "    def run(self, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"Démarrage de la simulation...\")\n",
    "        self.server.get_new_complete_batch(self.global_time)\n",
    "        while self.global_time < self.total_time_steps:\n",
    "            if verbose:\n",
    "                print(f\"\\n[Temps {self.global_time}]\")\n",
    "            finished_workers = []\n",
    "            for worker in self.in_progress_workers:\n",
    "                if worker.has_finished(self.global_time):\n",
    "                    mini_idx = worker.current_batch_info[1]\n",
    "                    gradients, batch_info = worker.compute_gradient(*self.server.current_mini_batches[mini_idx])\n",
    "                    self.server.register_gradient(gradients, batch_info, worker.worker_id,\n",
    "                                                  self.global_time, worker.processing_time)\n",
    "                    finished_workers.append(worker)\n",
    "            for worker in finished_workers:\n",
    "                self.in_progress_workers.remove(worker)\n",
    "                worker.current_batch_info = None\n",
    "            if len(self.server.received_gradients) == len(self.server.current_mini_batches):\n",
    "                self.server.aggregate_and_update()\n",
    "                self.server.get_new_complete_batch(self.global_time)\n",
    "                self.in_progress_workers = []\n",
    "            available_workers = [w for w in self.workers if w.is_available(self.global_time)\n",
    "                                 and w not in self.in_progress_workers]\n",
    "            for mini_idx in sorted(self.server.current_mini_batches.keys()):\n",
    "                if mini_idx not in self.server.received_gradients:\n",
    "                    if available_workers:\n",
    "                        worker = available_workers.pop(0)\n",
    "                        data, targets = self.server.current_mini_batches[mini_idx]\n",
    "                        worker.start_computation(self.global_time, (self.server.current_batch_id, mini_idx),\n",
    "                                                 data, targets)\n",
    "                        self.in_progress_workers.append(worker)\n",
    "                        if verbose:\n",
    "                            print(f\"[Temps {self.global_time}] Mini-batch {mini_idx} envoyé au worker {worker.worker_id}.\")\n",
    "                    else:\n",
    "                        break\n",
    "            self.global_time += 1\n",
    "        return self.server.evaluate(self.test_loader)\n",
    "\n",
    "    def plot_worker_availability(self):\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        for worker in self.workers:\n",
    "            color = \"tab:red\" if isinstance(worker, ByzantineWorker) else \"tab:blue\"\n",
    "            for (start, end) in worker.availability_periods:\n",
    "                ax.broken_barh([(start, end - start)], (worker.worker_id - 0.4, 0.8), facecolors=color)\n",
    "        ax.set_xlabel(\"Temps\")\n",
    "        ax.set_ylabel(\"ID du Worker\")\n",
    "        ax.set_title(\"Disponibilité des Workers\\n(bleu: honnêtes, rouge: byzantins)\")\n",
    "        ax.set_xlim(0, self.total_time_steps)\n",
    "        max_worker_id = max(worker.worker_id for worker in self.workers)\n",
    "        ax.set_ylim(0, max_worker_id + 1)\n",
    "        ax.set_yticks(range(1, max_worker_id + 1))\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Fonction d'exécution d'une expérience avec un taux donné de workers byzantins,\n",
    "# une stratégie d'agrégation principale et une méthode de pré-agrégation.\n",
    "def run_experiment(num_runs=5, total_time_steps=288, verbose=False, byzantine_ratio=0.0,\n",
    "                   aggregation_strategy='trimmed_mean', preagg=None):\n",
    "    accuracy_list = []\n",
    "    num_worker = 200\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        server = Server(train_dataset, batch_size=640, mini_batch_size=64, verbose=verbose,\n",
    "                        aggregation_strategy=aggregation_strategy, preagg=preagg)\n",
    "        workers = []\n",
    "        num_byzantine = int(num_worker * byzantine_ratio)\n",
    "        byzantine_indices = set(random.sample(range(1, num_worker + 1), num_byzantine))\n",
    "        for i in range(1, num_worker + 1):\n",
    "            processing_time = random.randint(2, 4)\n",
    "            if i in byzantine_indices:\n",
    "                availability = generate_gaussian_byzantine_availability(total_time_steps, worker_id=i, num_workers=num_worker,\n",
    "                                                                 min_processing_time=processing_time)\n",
    "                workers.append(ByzantineWorker(server.model, server.device, i, availability, processing_time))\n",
    "            else:\n",
    "                availability = generate_gaussian_availability(total_time_steps, worker_id=i, num_workers=num_worker, min_processing_time=processing_time)\n",
    "                workers.append(Worker(server.model, server.device, i, availability, processing_time))\n",
    "        simulator = DiscreteEventSimulator(server, workers, test_loader, total_time_steps=total_time_steps)\n",
    "        acc = simulator.run(verbose=verbose)\n",
    "        accuracy_list.append(acc)\n",
    "    return np.mean(accuracy_list)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Main : Pour chaque méthode de pré-agrégation, on regroupe dans un même plot les courbes pour toutes les stratégies d'agrégation.\n",
    "if __name__ == \"__main__\": # \n",
    "    preagg_methods = [\"NNM\",\"Bucketing\", \"ARC\", None ]  # None = pas de pré-agrégation\n",
    "    aggregator_methods = [\"trimmed_mean\", \"multi_krum\", \"geometric_median\", \"meamed\", \"average\"]\n",
    "    byzantine_ratios = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "    num_runs = 3\n",
    "    total_time_steps = 1000\n",
    "\n",
    "    for preagg in preagg_methods:\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        for agg in aggregator_methods:\n",
    "            results = []\n",
    "            for ratio in byzantine_ratios:\n",
    "                acc = run_experiment(num_runs=num_runs,\n",
    "                                     total_time_steps=total_time_steps,\n",
    "                                     verbose=False,\n",
    "                                     byzantine_ratio=ratio,\n",
    "                                     aggregation_strategy=agg,\n",
    "                                     preagg=preagg)\n",
    "                results.append(acc)\n",
    "                method_name = preagg if preagg is not None else \"NoPreAgg\"\n",
    "                print(f\"PreAgg: {method_name}, Aggregator: {agg}, Byzantine ratio: {ratio*100:.0f}%, Accuracy: {acc:.2f}%\")\n",
    "            plt.plot([r*100 for r in byzantine_ratios], results, marker='o', linewidth=2, markersize=8, label=agg)\n",
    "        title = f\"Pre-aggregation: {preagg if preagg is not None else 'None'}\"\n",
    "        plt.xlabel(\"Taux de workers byzantins (%)\", fontsize=14)\n",
    "        plt.ylabel(\"Accuracy moyenne (%)\", fontsize=14)\n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.legend(title=\"Aggregators\", fontsize=12, title_fontsize=13, loc=\"best\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        fname = f\"same_avail_accuracy_preagg_{preagg if preagg is not None else 'None'}.pdf\"\n",
    "        plt.savefig(fname, dpi=300)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
