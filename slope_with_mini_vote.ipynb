{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/__init__.py:2604\u001b[39m\n\u001b[32m   2600\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n\u001b[32m   2603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[32m-> \u001b[39m\u001b[32m2604\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[32m   2606\u001b[39m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mTORCH_CUDA_SANITIZER\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/_meta_registrations.py:6798\u001b[39m\n\u001b[32m   6794\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6795\u001b[39m                 _meta_lib_dont_use_me_use_register_meta.impl(op_overload, fn)\n\u001b[32m-> \u001b[39m\u001b[32m6798\u001b[39m \u001b[43mactivate_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/_meta_registrations.py:6795\u001b[39m, in \u001b[36mactivate_meta\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   6791\u001b[39m     _meta_lib_dont_use_me_use_register_meta_for_quantized.impl(\n\u001b[32m   6792\u001b[39m         op_overload, fn\n\u001b[32m   6793\u001b[39m     )\n\u001b[32m   6794\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m6795\u001b[39m     \u001b[43m_meta_lib_dont_use_me_use_register_meta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_overload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/library.py:349\u001b[39m, in \u001b[36mLibrary.impl\u001b[39m\u001b[34m(self, op_name, fn, dispatch_key, with_keyset)\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    342\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWe should not register a meta kernel directly to the operator \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    343\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m because it has a CompositeImplicitAutograd kernel in core.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    344\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Instead we should let the operator decompose, and ensure that we have meta kernels\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    345\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m for the base ops that it decomposes into.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    346\u001b[39m         )\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdispatch_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdispatch_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCompositeImplicitAutograd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_keyset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m _impls.add(key)\n\u001b[32m    357\u001b[39m \u001b[38;5;28mself\u001b[39m._op_impls.add(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:463\u001b[39m, in \u001b[36m_lock_unlock_module\u001b[39m\u001b[34m(name)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import byzfl\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "    intervals.sort()\n",
    "    merged = [intervals[0]]\n",
    "    for current in intervals[1:]:\n",
    "        last = merged[-1]\n",
    "        if current[0] <= last[1]:\n",
    "            merged[-1] = (last[0], max(last[1], current[1]))\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    return merged\n",
    "\n",
    "# Gaussian availability for honest and Byzantine workers\n",
    "\n",
    "def generate_gaussian_availability(total_time_steps, worker_id, num_workers,\n",
    "                                   std_dev_ratio=0.08, min_processing_time=3,\n",
    "                                   smoothing_window=10):\n",
    "    peak_means = [0.2, 0.5, 0.8]\n",
    "    std_dev = int(std_dev_ratio * total_time_steps)\n",
    "    availability_periods = []\n",
    "    for mean_ratio in peak_means:\n",
    "        mean = int(mean_ratio * total_time_steps)\n",
    "        start_time = int(np.random.normal(loc=mean, scale=std_dev))\n",
    "        start_time = max(0, min(start_time, total_time_steps - min_processing_time*3))\n",
    "        duration = random.randint(min_processing_time*3,\n",
    "                                  min(min_processing_time*5, total_time_steps - start_time))\n",
    "        availability_periods.append((start_time, start_time + duration))\n",
    "    merged = merge_intervals(availability_periods)\n",
    "    smoothed = [(max(0, s-smoothing_window), min(total_time_steps, e+smoothing_window)) for s,e in merged]\n",
    "    return merge_intervals(smoothed)\n",
    "\n",
    "def generate_gaussian_byzantine_availability(\n",
    "    total_time_steps, worker_id, num_workers,\n",
    "    std_dev_ratio=0.05, min_processing_time=3,\n",
    "    smoothing_window=10\n",
    "):\n",
    "    off_peak_means = [0.07, 0.9]\n",
    "    std_dev = int(std_dev_ratio * total_time_steps)\n",
    "    availability_periods = []\n",
    "    for mean_ratio in off_peak_means:\n",
    "        mean = int(mean_ratio * total_time_steps)\n",
    "        start = int(np.random.normal(loc=mean, scale=std_dev))\n",
    "        start = max(0, min(start, total_time_steps - min_processing_time*4))\n",
    "        duration = random.randint(\n",
    "            min_processing_time*4,\n",
    "            min(min_processing_time*8, total_time_steps - start)\n",
    "        )\n",
    "        availability_periods.append((start, start + duration))\n",
    "    merged = merge_intervals(availability_periods)\n",
    "    smoothed = [(max(0, s-smoothing_window), min(total_time_steps, e+smoothing_window)) for s,e in merged]\n",
    "    return merge_intervals(smoothed)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# LeNet-like model\n",
    "class LeNetLike(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetLike, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Batch manager\n",
    "class BatchManager:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = list(range(len(dataset)))\n",
    "        random.shuffle(self.indices)\n",
    "        self.sampler = SubsetRandomSampler(self.indices)\n",
    "        self.loader = iter(DataLoader(dataset, batch_size=batch_size, sampler=self.sampler))\n",
    "\n",
    "    def get_next_batch(self):\n",
    "        try:\n",
    "            return next(self.loader)\n",
    "        except StopIteration:\n",
    "            random.shuffle(self.indices)\n",
    "            self.sampler = SubsetRandomSampler(self.indices)\n",
    "            self.loader = iter(DataLoader(self.dataset, batch_size=self.batch_size, sampler=self.sampler))\n",
    "            return next(self.loader)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Worker classes\n",
    "class Worker:\n",
    "    def __init__(self, model, device, worker_id, availability_periods, processing_time):\n",
    "        self.is_byzantine = False\n",
    "        self.device = device\n",
    "        self.worker_id = worker_id\n",
    "        self.availability_periods = availability_periods\n",
    "        self.processing_time = processing_time\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.finish_time = None\n",
    "        self.current_batch_info = None\n",
    "        self._pending = None\n",
    "\n",
    "    def is_available(self, current_time):\n",
    "        for s,e in self.availability_periods:\n",
    "            if s <= current_time < e and (self.finish_time is None or current_time >= self.finish_time):\n",
    "                return (current_time + self.processing_time) <= e\n",
    "        return False\n",
    "\n",
    "    def start_computation(self, current_time, batch_info, data, targets):\n",
    "        if self.current_batch_info != batch_info:\n",
    "            self.current_batch_info = batch_info\n",
    "            self.finish_time = current_time + self.processing_time\n",
    "            self._pending = (data, targets)\n",
    "\n",
    "    def has_finished(self, current_time):\n",
    "        if self.finish_time == current_time and self._pending is not None:\n",
    "            data, targets = self._pending\n",
    "            self._pending = None\n",
    "            grads, bi = self.compute_gradient(data, targets)\n",
    "            return True, grads, bi\n",
    "        return False, None, None\n",
    "\n",
    "    def compute_gradient(self, data, targets):\n",
    "        data, targets = data.to(self.device), targets.to(self.device)\n",
    "        self.model.zero_grad()\n",
    "        outputs = self.model(data)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        grads = {n: p.grad.detach().clone() for n,p in self.model.named_parameters()}\n",
    "        return grads, self.current_batch_info\n",
    "\n",
    "    def update_model(self, state):\n",
    "        self.model.load_state_dict(state)\n",
    "\n",
    "class ByzantineWorker(Worker):\n",
    "    def __init__(self, model, device, worker_id, availability_periods, processing_time):\n",
    "        super().__init__(model, device, worker_id, availability_periods, processing_time)\n",
    "        self.is_byzantine = True\n",
    "\n",
    "    def compute_gradient(self, data, targets):\n",
    "        honest_grads, bi = super().compute_gradient(data, targets)\n",
    "        if random.random() < 0.5:\n",
    "            mag = random.uniform(0.5e6, 9e6)\n",
    "            corrupted = {n: torch.full_like(g, mag) for n,g in honest_grads.items()}\n",
    "        else:\n",
    "            flats, shapes = [], []\n",
    "            for n,g in honest_grads.items(): flats.append(g.view(-1)); shapes.append((n, g.shape))\n",
    "            g = torch.cat(flats)\n",
    "            r = torch.randn_like(g)\n",
    "            proj = (torch.dot(g,r)/torch.dot(g,g))*g if torch.dot(g,g)>0 else torch.zeros_like(g)\n",
    "            u = r - proj\n",
    "            if u.norm()>0: u = u*(g.norm()/u.norm())\n",
    "            theta = random.uniform(math.pi/2, 3*math.pi/2)\n",
    "            vec = math.cos(theta)*g + math.sin(theta)*u\n",
    "            corrupted = {}\n",
    "            idx = 0\n",
    "            for n,sh in shapes:\n",
    "                num = int(torch.tensor(sh).prod().item())\n",
    "                corrupted[n] = vec[idx:idx+num].view(sh); idx+=num\n",
    "        return corrupted, bi\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "class Server:\n",
    "    def __init__(self, dataset, batch_size=64, mini_batch_size=16, lr=0.005,\n",
    "                 verbose=False, aggregation_strategy='majority_vote', vote_k=3, preagg=None, f=0):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = LeNetLike().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.batch_manager = BatchManager(dataset, batch_size)\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.current_batch_id = 0\n",
    "        self.received_gradients = {}\n",
    "        self.verbose = verbose\n",
    "        self.aggregation_strategy = aggregation_strategy\n",
    "        self.vote_k = vote_k\n",
    "        self.preagg = preagg\n",
    "        self.f = f\n",
    "        self.total_raw_compute_time = 0\n",
    "        self.total_useful_compute_time = 0\n",
    "        self.total_waste_compute_time = 0\n",
    "        self.current_full_batch_compute_time = 0\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _gradients_are_equal(self, g1, g2, rtol=1e-3, atol=1e-8):\n",
    "        for k in g1:\n",
    "            if not torch.allclose(g1[k], g2[k], rtol=rtol, atol=atol): return False\n",
    "        return True\n",
    "\n",
    "    def get_new_complete_batch(self, current_time):\n",
    "        data, targets = self.batch_manager.get_next_batch()\n",
    "        self.current_batch_id += 1\n",
    "        self.current_full_batch_compute_time = 0\n",
    "        mbs = [(data[i:i+self.mini_batch_size], targets[i:i+self.mini_batch_size])\n",
    "               for i in range(0, data.size(0), self.mini_batch_size)]\n",
    "        self.current_mini_batches = {i: mbs[i] for i in range(len(mbs))}\n",
    "        self.received_gradients = {}\n",
    "        if self.verbose:\n",
    "            print(f\"[t={current_time}] New batch {self.current_batch_id}\")\n",
    "\n",
    "    def register_gradient(self, grads, batch_info, wid, current_time, pt, is_byzantine=False):\n",
    "        bid, mid = batch_info\n",
    "        if bid != self.current_batch_id: return\n",
    "        if self.aggregation_strategy == 'majority_vote':\n",
    "            lst = self.received_gradients.setdefault(mid, [])\n",
    "            if len(lst) < self.vote_k:\n",
    "                lst.append(grads)\n",
    "                self.total_raw_compute_time += pt\n",
    "                self.current_full_batch_compute_time += pt\n",
    "                if self.verbose:\n",
    "                    tag = ' (B)' if is_byzantine else ''\n",
    "                    print(f\"[t={current_time}] Grad {mid} w{wid}{tag}\")\n",
    "        else:\n",
    "            if mid in self.received_gradients: return\n",
    "            self.received_gradients[mid] = grads\n",
    "            self.total_raw_compute_time += pt\n",
    "            self.current_full_batch_compute_time += pt\n",
    "            if self.verbose:\n",
    "                tag = ' (B)' if is_byzantine else ''\n",
    "                print(f\"[t={current_time}] Grad {mid} w{wid}{tag}\")\n",
    "\n",
    "    def aggregate_and_update(self):\n",
    "        # wait condition\n",
    "        if self.aggregation_strategy == 'majority_vote':\n",
    "            if any(len(self.received_gradients.get(mid, [])) < self.vote_k\n",
    "                   for mid in self.current_mini_batches):\n",
    "                if self.verbose: print(\"⏳ waiting votes...\")\n",
    "                return False\n",
    "        else:\n",
    "            if len(self.received_gradients) != len(self.current_mini_batches):\n",
    "                if self.verbose: print(\"⚠️ Not enough grads.\")\n",
    "                return False\n",
    "        # preagg\n",
    "        pre = None\n",
    "        if self.preagg == 'NNM': pre = byzfl.NNM(f=self.f)\n",
    "        elif self.preagg == 'Bucketing': pre = byzfl.Bucketing(s=1)\n",
    "        elif self.preagg == 'ARC': pre = byzfl.ARC(f=self.f)\n",
    "        # aggregate\n",
    "        aggs = {}\n",
    "        if self.aggregation_strategy == 'majority_vote':\n",
    "            winners = {}\n",
    "            for mid, lst in self.received_gradients.items():\n",
    "                best, bc = None, -1\n",
    "                for g in lst:\n",
    "                    c = sum(self._gradients_are_equal(g,h) for h in lst)\n",
    "                    if c>bc: best, bc = g, c\n",
    "                winners[mid] = best\n",
    "            for name,param in self.model.named_parameters():\n",
    "                M = torch.stack([winners[mid][name].view(-1)\n",
    "                                 for mid in sorted(self.current_mini_batches)])\n",
    "                aggs[name] = M.mean(dim=0).view(param.size())\n",
    "        else:\n",
    "            for name,param in self.model.named_parameters():\n",
    "                Gs = [self.received_gradients[i][name]\n",
    "                      for i in sorted(self.current_mini_batches)]\n",
    "                G = torch.stack(Gs).view(len(Gs), -1)\n",
    "                if pre: G = pre(G)\n",
    "                if self.aggregation_strategy=='trimmed_mean':\n",
    "                    flat = byzfl.TrMean(f=max(self.f,(len(Gs)//2)-1))(G)\n",
    "                else:\n",
    "                    flat = byzfl.Average()(G)\n",
    "                aggs[name] = flat.view(param.size())\n",
    "        # apply\n",
    "        self.optimizer.zero_grad()\n",
    "        for name,param in self.model.named_parameters(): param.grad = aggs[name]\n",
    "        self.optimizer.step()\n",
    "        if self.verbose:\n",
    "            print(f\"Updated model batch {self.current_batch_id}\")\n",
    "        return True\n",
    "\n",
    "    def evaluate_acc(self, loader):\n",
    "        self.model.eval()\n",
    "        correct=0; total=0\n",
    "        with torch.no_grad():\n",
    "            for x,t in loader:\n",
    "                x,t=x.to(self.device),t.to(self.device)\n",
    "                pred=self.model(x).argmax(dim=1)\n",
    "                total+=t.size(0)\n",
    "                correct+=(pred==t).sum().item()\n",
    "        self.model.train()\n",
    "        return 100*correct/total\n",
    "\n",
    "    def evaluate_loss(self, loader):\n",
    "        self.model.eval()\n",
    "        tot_loss=0; tot=0\n",
    "        with torch.no_grad():\n",
    "            for x,t in loader:\n",
    "                x,t=x.to(self.device),t.to(self.device)\n",
    "                l=self.criterion(self.model(x), t)\n",
    "                b=x.size(0)\n",
    "                tot_loss+=l.item()*b; tot+=b\n",
    "        self.model.train()\n",
    "        return tot_loss/tot\n",
    "\n",
    "    def report_compute_metrics(self):\n",
    "        tot=self.total_raw_compute_time\n",
    "        use=self.total_useful_compute_time\n",
    "        waste=self.total_waste_compute_time\n",
    "        rate=100*waste/tot if tot else 0\n",
    "        print(f\"Raw: {tot}, Useful: {use}, Waste: {waste}, Rate: {rate:.2f}%\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "def create_workers(num_worker, byz_idx, steps, server):\n",
    "    workers=[]\n",
    "    for i in range(1,num_worker+1):\n",
    "        pt = random.randint(2,10)\n",
    "        if i in byz_idx:\n",
    "            av = generate_gaussian_byzantine_availability(steps,i,num_worker,min_processing_time=pt)\n",
    "            w=ByzantineWorker(server.model,server.device,i,av,pt)\n",
    "        else:\n",
    "            av = generate_gaussian_availability(steps,i,num_worker,min_processing_time=pt)\n",
    "            w=Worker(server.model,server.device,i,av,pt)\n",
    "        workers.append(w)\n",
    "    return workers\n",
    "\n",
    "class SafePointSimulator:\n",
    "    def __init__(self, server, workers, val_loader, test_loader, total_time_steps=1000, group_size=3):\n",
    "        self.server=server\n",
    "        self.workers=workers\n",
    "        self.val_loader=val_loader\n",
    "        self.test_loader=test_loader\n",
    "        self.total_time_steps=total_time_steps\n",
    "        self.group_size=group_size\n",
    "        self.global_time=0\n",
    "        self.in_progress=[]\n",
    "        self.server.get_new_complete_batch(self.global_time)\n",
    "        self.plot_worker_availability()\n",
    "\n",
    "    def plot_worker_availability(self):\n",
    "        num=len(self.workers)\n",
    "        fig,ax=plt.subplots(figsize=(12,max(6,min(0.05*num,20))))\n",
    "        for w in self.workers:\n",
    "            c='tab:red' if w.is_byzantine else 'tab:blue'\n",
    "            for s,e in w.availability_periods:\n",
    "                ax.broken_barh([(s,e-s)],(w.worker_id-0.4,0.8),facecolors=c)\n",
    "        ax.set_xlabel('Time'); ax.set_ylabel('Worker ID')\n",
    "        ax.set_title('Availability (blue: honest, red: byz)')\n",
    "        ax.set_xlim(0,self.total_time_steps)\n",
    "        ax.set_ylim(0,max(w.worker_id for w in self.workers)+1)\n",
    "        ax.grid(True,axis='y',linestyle='--',alpha=0.2)\n",
    "        from matplotlib.patches import Patch\n",
    "        ax.legend([Patch(facecolor='tab:blue'),Patch(facecolor='tab:red')],['Honest','Byz'],loc='upper right')\n",
    "        plt.tight_layout(); plt.show()\n",
    "        # cumulative\n",
    "        honest=[0]*self.total_time_steps\n",
    "        byz=[0]*self.total_time_steps\n",
    "        for w in self.workers:\n",
    "            tgt = byz if w.is_byzantine else honest\n",
    "            for s,e in w.availability_periods:\n",
    "                for t in range(s,min(e,self.total_time_steps)):\n",
    "                    tgt[t]+=1\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(range(self.total_time_steps), honest, linewidth=2)\n",
    "        plt.plot(range(self.total_time_steps), byz, linewidth=2)\n",
    "        plt.xlabel('Time'); plt.ylabel('Count')\n",
    "        plt.title('Availability over time')\n",
    "        plt.legend(['Honest','Byz']); plt.grid('--',alpha=0.6)\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    def run(self, verbose=False, eval_interval=10):\n",
    "        # initial safe point\n",
    "        safe_m = copy.deepcopy(self.server.model.state_dict())\n",
    "        safe_o = copy.deepcopy(self.server.optimizer.state_dict())\n",
    "        safe_acc = self.server.evaluate_acc(self.val_loader)\n",
    "        safe_loss= self.server.evaluate_loss(self.val_loader)\n",
    "        print(f\"Safe init — Acc: {safe_acc:.2f}%, Loss: {safe_loss:.4f}\")\n",
    "        #global_accs=[safe_acc]\n",
    "        #global_losses=[safe_loss]\n",
    "        group_updates=[]; group_losses=[]; group_durs=[]\n",
    "        count=0\n",
    "        tpts=[]; accs=[]\n",
    "        k=self.server.vote_k\n",
    "        while self.global_time < self.total_time_steps:\n",
    "            # collect finished\n",
    "            for w in self.in_progress[:]:\n",
    "                done, grads, bi = w.has_finished(self.global_time)\n",
    "                if not done: continue\n",
    "                self.server.register_gradient(grads, bi, w.worker_id, self.global_time, w.processing_time, w.is_byzantine)\n",
    "                self.in_progress.remove(w)\n",
    "            # aggregate\n",
    "            if self.server.aggregate_and_update():\n",
    "                acc = self.server.evaluate_acc(self.val_loader)\n",
    "                vl = self.server.evaluate_loss(self.val_loader)\n",
    "                print(f\"[t={self.global_time}] Batch {self.server.current_batch_id} → Acc: {acc:.2f}%, Loss: {vl:.4f}\")\n",
    "                group_updates.append(acc) \n",
    "                group_losses.append(vl)\n",
    "                group_durs.append(self.server.current_full_batch_compute_time)\n",
    "                count+=1\n",
    "                self.server.get_new_complete_batch(self.global_time)\n",
    "                self.in_progress.clear()\n",
    "                if count==self.group_size:\n",
    "                    eps_acc = 1e-5\n",
    "                    x_acc = np.arange(len(group_updates))\n",
    "                    slope_acc, _ = np.polyfit(x_acc, np.array(group_updates), 1)\n",
    "                    print(slope_acc)\n",
    "                    slope_acc = slope_acc if abs(slope_acc) > eps_acc else 0.0\n",
    "                    delta_acc = group_updates[-1] - safe_acc\n",
    "                    \n",
    "                    print(slope_acc)\n",
    "                    ok_acc = (slope_acc > 0) and (delta_acc > 0)\n",
    "                    # 3b) pente linéaire de -loss locale + gain absolu\n",
    "                    eps_loss = 1e-5\n",
    "                    x_loss = np.arange(len(group_losses))\n",
    "                    slope_loss, _ = np.polyfit(x_loss, -np.array(group_losses), 1)\n",
    "                    print(slope_loss)\n",
    "                    slope_loss = slope_loss if abs(slope_loss) > eps_loss else 0.0\n",
    "                    delta_loss = safe_loss - group_losses[-1]\n",
    "                    print(slope_loss)\n",
    "                    ok_loss = (slope_loss > 0) and (delta_loss > 0)\n",
    "\n",
    "                    # 3c) décision combinée : on exige progression sur les deux\n",
    "                    accept = ok_acc and ok_loss\n",
    "                    total_t = sum(group_durs)\n",
    "                    if accept:\n",
    "                        safe_m=copy.deepcopy(self.server.model.state_dict())\n",
    "                        safe_o=copy.deepcopy(self.server.optimizer.state_dict())\n",
    "                        safe_acc=group_updates[-1]\n",
    "                        safe_loss=group_losses[-1]\n",
    "                        self.server.total_useful_compute_time += total_t\n",
    "                        print(f\"ACCEPT — Acc {safe_acc:.2f}%, Loss {safe_loss:.4f}\")\n",
    "                    else:\n",
    "                        self.server.model.load_state_dict(safe_m)\n",
    "                        self.server.optimizer.load_state_dict(safe_o)\n",
    "                        self.server.total_waste_compute_time += total_t\n",
    "                        print(f\"ROLLBACK — Acc {safe_acc:.2f}%, Loss {safe_loss:.4f}\")\n",
    "                    group_updates.clear(); group_losses.clear(); group_durs.clear(); count=0\n",
    "            # assign new\n",
    "            avail=[w for w in self.workers if w.is_available(self.global_time) and w not in self.in_progress]\n",
    "            for mid,mb in self.server.current_mini_batches.items():\n",
    "                if self.server.aggregation_strategy=='majority_vote':\n",
    "                    got = len(self.server.received_gradients.get(mid, []))\n",
    "                else:\n",
    "                    got = 1 if mid in self.server.received_gradients else 0\n",
    "                while got < k and avail:\n",
    "                    w = avail.pop(0)\n",
    "                    w.update_model(copy.deepcopy(self.server.model.state_dict()))\n",
    "                    data,tgt = mb\n",
    "                    w.start_computation(self.global_time, (self.server.current_batch_id, mid), data, tgt)\n",
    "                    self.in_progress.append(w)\n",
    "                    got +=1\n",
    "            # periodic test eval\n",
    "            if self.global_time % eval_interval==0:\n",
    "                test_acc = self.server.evaluate_acc(self.test_loader)\n",
    "                tpts.append(self.global_time); accs.append(test_acc)\n",
    "            self.global_time+=1\n",
    "        # restore\n",
    "        self.server.model.load_state_dict(safe_m)\n",
    "        self.server.optimizer.load_state_dict(safe_o)\n",
    "        final = self.server.evaluate_acc(self.test_loader)\n",
    "        print(f\"Final acc: {final:.2f}%\")\n",
    "        plt.figure(figsize=(10,5)); plt.plot(tpts, accs, linewidth=2)\n",
    "        plt.xlabel('Time'); plt.ylabel('Accuracy (%)'); plt.title('Test Acc over time')\n",
    "        plt.grid('--',alpha=0.6); plt.tight_layout(); plt.show()\n",
    "        return tpts, accs, final\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "def run_experiment(num_cycles=1, num_runs=1, steps_per_cycle=1000,\n",
    "                   byzantine_ratio=0.0, aggregation_strategy='majority_vote', vote_k=3, preagg=None):\n",
    "    trajectories = {}\n",
    "    num_worker = 200\n",
    "    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    full_train = datasets.MNIST('./data', train=True, download=True, transform=trans)\n",
    "    test_ds = datasets.MNIST('./data', train=False, download=True, transform=trans)\n",
    "    n_total=len(full_train)\n",
    "    n_val=int(0.2*n_total)\n",
    "    perm=torch.randperm(n_total)\n",
    "    val_idx, train_idx = perm[:n_val].tolist(), perm[n_val:].tolist()\n",
    "    val_loader = DataLoader(full_train, batch_size=200, sampler=SubsetRandomSampler(val_idx))\n",
    "    test_loader=DataLoader(test_ds, batch_size=120, shuffle=False)\n",
    "    for ratio in [byzantine_ratio]:\n",
    "        server = Server(full_train, batch_size=640, mini_batch_size=64,\n",
    "                        verbose=False, aggregation_strategy=aggregation_strategy,\n",
    "                        vote_k=vote_k, preagg=preagg)\n",
    "        byz = set(random.sample(range(1,num_worker+1), int(num_worker*ratio)))\n",
    "        workers = create_workers(num_worker, byz, steps_per_cycle, server)\n",
    "        sim = SafePointSimulator(server, workers, val_loader, test_loader,\n",
    "                                 total_time_steps=steps_per_cycle, group_size=10)\n",
    "        tps, accs, final = sim.run(verbose=False, eval_interval=10)\n",
    "        trajectories[int(ratio*100)] = (tps, accs)\n",
    "        server.report_compute_metrics()\n",
    "    # plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for r,(t,a) in trajectories.items():\n",
    "        plt.plot(t, a, linewidth=2, label=f\"{r}% Byzantine\")\n",
    "    plt.xlabel('Time'); plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Accuracy over time')\n",
    "    plt.legend(title='Byzantine %')\n",
    "    plt.grid('--',alpha=0.6); plt.tight_layout(); plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_experiment(steps_per_cycle=1000, byzantine_ratio=0.4,\n",
    "                   aggregation_strategy='majority_vote', vote_k=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
