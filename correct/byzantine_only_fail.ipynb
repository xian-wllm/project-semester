{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "    intervals.sort()\n",
    "    merged = [intervals[0]]\n",
    "    for current in intervals[1:]:\n",
    "        last = merged[-1]\n",
    "        if current[0] <= last[1]:\n",
    "            merged[-1] = (last[0], max(last[1], current[1]))\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    return merged\n",
    "\n",
    "def generate_dynamic_availability(total_time_steps, worker_id, num_workers, max_periods=5, min_processing_time=2):\n",
    "    # On d√©finit le centre de la simulation\n",
    "    center = total_time_steps / 2\n",
    "    # On fixe un offset maximum (par exemple total_time_steps / 8)\n",
    "    max_offset = total_time_steps / 8\n",
    "    # Calcul de l'offset pour chaque worker de fa√ßon sym√©trique\n",
    "    if num_workers % 2 == 1:\n",
    "        mid = (num_workers + 1) / 2\n",
    "        offset = ((worker_id - mid) / mid) * max_offset\n",
    "    else:\n",
    "        mid = num_workers / 2\n",
    "        offset = ((worker_id - mid - 0.5) / mid) * max_offset\n",
    "    base_mean = center + offset\n",
    "    # On ajoute un petit al√©a pour √©viter une r√©gularit√© trop marqu√©e\n",
    "    mean = int(np.clip(base_mean + random.randint(-total_time_steps // 20, total_time_steps // 20), 0, total_time_steps - min_processing_time))\n",
    "    std_dev = total_time_steps / 6\n",
    "    availability_periods = []\n",
    "    num_periods = random.randint(1, max_periods)\n",
    "    for _ in range(num_periods):\n",
    "        start_time = int(np.clip(np.random.normal(mean, std_dev), 0, total_time_steps - min_processing_time))\n",
    "        duration = max(min_processing_time, int(np.abs(np.random.normal(3, 2))))\n",
    "        end_time = min(start_time + duration, total_time_steps)\n",
    "        if end_time - start_time >= min_processing_time:\n",
    "            availability_periods.append((start_time, end_time))\n",
    "    return merge_intervals(availability_periods)\n",
    "\n",
    "\n",
    "# D√©finition du mod√®le de r√©seau de neurones\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Gestionnaire de mini-batches avec SubsetRandomSampler\n",
    "class BatchManager:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = list(range(len(dataset)))\n",
    "        random.shuffle(self.indices)\n",
    "        self.sampler = SubsetRandomSampler(self.indices)\n",
    "        self.loader = iter(DataLoader(self.dataset, batch_size=self.batch_size, sampler=self.sampler))\n",
    "    def get_next_batch(self):\n",
    "        try:\n",
    "            return next(self.loader)\n",
    "        except StopIteration:\n",
    "            random.shuffle(self.indices)\n",
    "            self.sampler = SubsetRandomSampler(self.indices)\n",
    "            self.loader = iter(DataLoader(self.dataset, batch_size=self.batch_size, sampler=self.sampler))\n",
    "            return next(self.loader)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Classe Worker : g√®re le calcul local des gradients\n",
    "class Worker:\n",
    "    def __init__(self, model, device, worker_id, availability_periods, processing_time):\n",
    "        self.device = device\n",
    "        self.worker_id = worker_id\n",
    "        self.availability_periods = availability_periods\n",
    "        self.processing_time = processing_time\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.finish_time = None\n",
    "        self.current_batch_id = None\n",
    "\n",
    "    def is_available(self, current_time):\n",
    "        for start_time, end_time in self.availability_periods:\n",
    "            if start_time <= current_time < end_time:\n",
    "                if self.finish_time is None or current_time >= self.finish_time:\n",
    "                    return (current_time + self.processing_time) <= end_time\n",
    "        return False\n",
    "\n",
    "    def start_computation(self, current_time, batch_id, data, targets):\n",
    "        if self.current_batch_id is None or self.current_batch_id != batch_id:\n",
    "            self.current_batch_id = batch_id\n",
    "            self.finish_time = current_time + self.processing_time\n",
    "            #print(f\"üöÄ [T={current_time}] Worker {self.worker_id} commence un NOUVEAU calcul sur batch {batch_id}.\")\n",
    "            return self.compute_gradient(data, targets)\n",
    "        return None\n",
    "\n",
    "    def has_finished(self, current_time):\n",
    "        return self.finish_time == current_time\n",
    "\n",
    "    def compute_gradient(self, data, targets):\n",
    "        data, targets = data.to(self.device), targets.to(self.device)\n",
    "        self.model.zero_grad()\n",
    "        outputs = self.model(data)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        gradients = {name: param.grad.detach().clone() for name, param in self.model.named_parameters()}\n",
    "        return gradients, self.current_batch_id\n",
    "    def update_model(self, model_state_dict):\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Serveur central : g√®re l'application des gradients\n",
    "class Server:\n",
    "    def __init__(self, dataset, batch_size=64, lr=0.001, max_contribution_ratio=0.5):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SimpleNN().to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.batch_manager = BatchManager(dataset, batch_size)\n",
    "        self.current_batch = None\n",
    "        self.current_batch_id = 0\n",
    "        self.model_updated = True\n",
    "        self.worker_contributions = {}   # Nombre de contributions utiles par worker\n",
    "        self.wasted_gradients = {}       # Temps perdu par worker (en unit√©s de temps discret)\n",
    "        self.batch_send_times = {}       # Enregistre le temps d'envoi de chaque batch\n",
    "        self.total_wasted_time = 0       # Temps total de computing perdu\n",
    "        self.max_contribution_ratio = max_contribution_ratio  # Seuil maximal de contribution par worker\n",
    "\n",
    "    def update_global_model(self, worker_gradients, batch_id, worker_id, current_time):\n",
    "        # Calculer la contribution totale actuelle (si non nul)\n",
    "        total_contrib = sum(self.worker_contributions.values()) if self.worker_contributions else 0\n",
    "        \n",
    "        # Appliquer le contr√¥le du ratio uniquement si le seuil est inf√©rieur √† 1\n",
    "        if self.max_contribution_ratio < 1 and total_contrib > 0:\n",
    "            worker_ratio = self.worker_contributions.get(worker_id, 0) / total_contrib\n",
    "            if worker_ratio >= self.max_contribution_ratio:\n",
    "                send_time = self.batch_send_times.get(batch_id, current_time)\n",
    "                wasted = current_time - send_time\n",
    "                self.wasted_gradients[worker_id] = self.wasted_gradients.get(worker_id, 0) + wasted\n",
    "                self.total_wasted_time += wasted\n",
    "                return\n",
    "\n",
    "        if worker_gradients is None or batch_id != self.current_batch_id:\n",
    "            send_time = self.batch_send_times.get(batch_id, current_time)\n",
    "            wasted = current_time - send_time\n",
    "            self.wasted_gradients[worker_id] = self.wasted_gradients.get(worker_id, 0) + wasted\n",
    "            self.total_wasted_time += wasted\n",
    "            return\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.grad = worker_gradients[name]\n",
    "        self.optimizer.step()\n",
    "        self.worker_contributions[worker_id] = self.worker_contributions.get(worker_id, 0) + 1\n",
    "        self.model_updated = True\n",
    "\n",
    "\n",
    "    def compute_wasted_computing_time(self):\n",
    "        return self.total_wasted_time\n",
    "\n",
    "    def print_worker_statistics(self):\n",
    "        print(\"\\n R√©sum√© des contributions utiles des workers :\")\n",
    "        sorted_contrib = sorted(self.worker_contributions.items(), key=lambda x: x[1], reverse=True)\n",
    "        for worker_id, count in sorted_contrib:\n",
    "            print(f\"üîπ Worker {worker_id} a contribu√© {count} fois √† la mise √† jour du mod√®le.\")\n",
    "        print(\"\\n R√©sum√© des wasted computing (temps perdu) :\")\n",
    "        sorted_wasted = sorted(self.wasted_gradients.items(), key=lambda x: x[1], reverse=True)\n",
    "        for worker_id, wasted_time in sorted_wasted:\n",
    "            print(f\"üîπ Worker {worker_id} a perdu {wasted_time} unit√©s de temps de computing.\")\n",
    "        wasted_time_total = self.compute_wasted_computing_time()\n",
    "        print(f\"\\n Temps total de computing perdu : {wasted_time_total} unit√©s de temps.\")\n",
    "\n",
    "    def get_batch_for_worker(self, current_time):\n",
    "        if self.model_updated:\n",
    "            self.current_batch = self.batch_manager.get_next_batch()\n",
    "            self.current_batch_id += 1\n",
    "            self.batch_send_times[self.current_batch_id] = current_time\n",
    "            self.model_updated = False\n",
    "        return self.current_batch, self.current_batch_id\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, targets in test_loader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def plot_wasted_computing(self):\n",
    "        \"\"\"Affiche un histogramme des unit√©s de temps de computing perdues par worker.\"\"\"\n",
    "        if not self.wasted_gradients:\n",
    "            return\n",
    "\n",
    "        worker_ids = list(self.wasted_gradients.keys())\n",
    "        wasted_times = list(self.wasted_gradients.values())\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(worker_ids, wasted_times, color='red', alpha=0.7)\n",
    "        plt.xlabel(\"ID du Worker\")\n",
    "        plt.ylabel(\"Unit√©s de Temps de Computing Perdues\")\n",
    "        plt.title(\"Histogramme des Computing Perdus par Worker\")\n",
    "\n",
    "        plt.xticks(worker_ids)\n",
    "\n",
    "        # Ajustement automatique des ticks de Y\n",
    "        plt.gca().yaxis.set_major_locator(ticker.MaxNLocator(nbins=6, integer=True))\n",
    "\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#### Simulation d'apprentissage f√©d√©r√© avec affichage des statistiques et disponibilit√©s\n",
    "class DiscreteEventSimulator:\n",
    "    def __init__(self, server, workers, test_loader, total_time_steps=100):\n",
    "        self.server = server\n",
    "        self.workers = workers\n",
    "        self.test_loader = test_loader\n",
    "        self.total_time_steps = total_time_steps\n",
    "        self.global_time = 0\n",
    "        self.worker_computing = []\n",
    "        \n",
    "    def plot_worker_availability(self):\n",
    "        num_workers = len(self.workers)\n",
    "        total_time_steps = self.total_time_steps\n",
    "        availability_data = [(worker.worker_id, worker.availability_periods) for worker in self.workers]\n",
    "        time_grid = np.zeros((num_workers, total_time_steps))\n",
    "        for worker_id, periods in availability_data:\n",
    "            for start, end in periods:\n",
    "                time_grid[worker_id - 1, start:end] = 1\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(np.flipud(time_grid), aspect=\"auto\", cmap=\"Blues\", interpolation=\"nearest\")\n",
    "        plt.xlabel(\"Temps (Steps)\")\n",
    "        plt.ylabel(\"Workers\")\n",
    "        plt.title(\"Disponibilit√© des Travailleurs avant la Simulation\")\n",
    "\n",
    "        # Ajustement des ticks pour correspondre aux workers r√©els\n",
    "        plt.xticks(range(0, total_time_steps, max(1, total_time_steps // 10)))\n",
    "        plt.yticks(range(num_workers), [f\"Worker {i}\" for i in reversed(range(1, num_workers + 1))])\n",
    "\n",
    "        plt.colorbar(label=\"Disponibilit√© (1 = Disponible, 0 = Indisponible)\")\n",
    "        plt.show()\n",
    "\n",
    "        availability_counts = np.sum(time_grid, axis=0)\n",
    "        mean_time = total_time_steps / 2\n",
    "        std_dev_time = total_time_steps / 6\n",
    "        x = np.arange(total_time_steps)\n",
    "        normal_distribution = num_workers * (1 / (std_dev_time * np.sqrt(2 * np.pi))) * \\\n",
    "                              np.exp(-((x - mean_time) ** 2) / (2 * std_dev_time ** 2))\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(x, availability_counts, marker='o', linestyle='-', label=\"Disponibilit√© totale r√©elle\", color='blue')\n",
    "        plt.plot(x, normal_distribution, linestyle='--', label=\"Approximation Normale\", color='red')\n",
    "        plt.xlabel(\"Temps (Steps)\")\n",
    "        plt.ylabel(\"Nombre de Travailleurs Disponibles\")\n",
    "        plt.title(\"Comparaison de la Disponibilit√© des Travailleurs avec une Loi Normale\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    def run(self):\n",
    "        #print(\" Affichage des disponibilit√©s des workers avant la simulation...\")\n",
    "        #self.plot_worker_availability()\n",
    "        #print(\" D√©marrage de la simulation...\")\n",
    "        for worker in self.workers:\n",
    "            #print(f\" Worker {worker.worker_id} disponible aux p√©riodes {worker.availability_periods} | Temps de traitement : {worker.processing_time}\")\n",
    "            pass\n",
    "        while self.global_time < self.total_time_steps:\n",
    "            #print(f\"\\n Temps {self.global_time}\")\n",
    "            newly_finished_workers = [worker for worker in self.worker_computing if worker.has_finished(self.global_time)]\n",
    "            for worker in newly_finished_workers:\n",
    "                self.worker_computing.remove(worker)\n",
    "                batch, _ = self.server.get_batch_for_worker(self.global_time)\n",
    "                data, targets = batch\n",
    "                gradients, batch_id = worker.compute_gradient(data, targets)\n",
    "                self.server.update_global_model(gradients, batch_id, worker.worker_id, self.global_time)\n",
    "            available_workers = [worker for worker in self.workers if worker.is_available(self.global_time) and worker not in self.worker_computing]\n",
    "            for worker in available_workers:\n",
    "                batch, batch_id = self.server.get_batch_for_worker(self.global_time)\n",
    "                data, targets = batch\n",
    "                worker.start_computation(self.global_time, batch_id, data, targets)\n",
    "                self.worker_computing.append(worker)\n",
    "            if self.global_time % 5 == 0:\n",
    "                accuracy = self.server.evaluate(self.test_loader)\n",
    "                #print(f\" [T={self.global_time}] Pr√©cision du mod√®le: {accuracy:.2f}%\")\n",
    "            self.global_time += 1\n",
    "        print(\" Simulation termin√©e !\")\n",
    "        #self.server.print_worker_statistics()\n",
    "        #self.server.plot_wasted_computing()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByzantineWorker(Worker):\n",
    "    def compute_gradient(self, data, targets):\n",
    "        #print(f\"‚ö†Ô∏è [Byzantin] Worker {self.worker_id} envoie un gradient corrompu !\")\n",
    "        corrupted_gradients = {name: torch.full_like(param, 1e10) for name, param in self.model.named_parameters()}\n",
    "        return corrupted_gradients, self.current_batch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## librairie\n",
    "\n",
    "BIZFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chargement des donn√©es\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# # Initialisation du serveur et des workers\n",
    "# server = Server(train_dataset)\n",
    "# workers = []\n",
    "# num_workers = 10\n",
    "# total_time_steps = 51  \n",
    "# byzantine_percentage = 20  # Pourcentage de travailleurs byzantins\n",
    "# num_byzantine = int((byzantine_percentage / 100) * num_workers)\n",
    "\n",
    "# workers = []\n",
    "# for i in range(num_workers):\n",
    "#     processing_time = random.randint(2, 4)\n",
    "#     availability = generate_dynamic_availability(total_time_steps, i, num_workers, max_periods=5, min_processing_time=processing_time)\n",
    "    \n",
    "#     if i < num_byzantine:\n",
    "#         workers.append(ByzantineWorker(server.model, server.device, i, availability, processing_time))\n",
    "#     else:\n",
    "#         workers.append(Worker(server.model, server.device, i, availability, processing_time))\n",
    "\n",
    "\n",
    "# simulator = DiscreteEventSimulator(server, workers, test_loader, total_time_steps=total_time_steps)\n",
    "# simulator.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Simulation termin√©e !\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Lancer la simulation\u001b[39;00m\n\u001b[32m     39\u001b[39m simulator = DiscreteEventSimulator(server, workers, test_loader, total_time_steps=total_time_steps)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43msimulator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Stocker l'accuracy finale\u001b[39;00m\n\u001b[32m     43\u001b[39m final_accuracy = server.evaluate(test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 298\u001b[39m, in \u001b[36mDiscreteEventSimulator.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.worker_computing.append(worker)\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.global_time % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     accuracy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m#print(f\" [T={self.global_time}] Pr√©cision du mod√®le: {accuracy:.2f}%\")\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;28mself\u001b[39m.global_time += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mServer.evaluate\u001b[39m\u001b[34m(self, test_loader)\u001b[39m\n\u001b[32m    195\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/ma2/project-semester/.env/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:917\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    912\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    913\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    914\u001b[39m     )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m dtype = tensor.dtype\n\u001b[32m    920\u001b[39m mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Liste des pourcentages de workers byzantins √† tester\n",
    "byzantine_percentages = [0, 10, 20, 50]\n",
    "max_contribution_per_worker_values = np.arange(0.1, 1.1 , 0.1)\n",
    "num_runs = 1  # Nombre de runs pour chaque configuration\n",
    "\n",
    "# Param√®tres fixes\n",
    "num_workers = 10\n",
    "total_time_steps = 288  # 1 step is 5 min\n",
    "\n",
    "# Stockage des r√©sultats\n",
    "results = {}\n",
    "\n",
    "for max_contribution_per_worker in max_contribution_per_worker_values:\n",
    "    mean_accuracies = []\n",
    "\n",
    "    for byzantine_percentage in byzantine_percentages:\n",
    "        accuracies = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            # Cr√©ation du serveur avec la contribution max en param√®tre\n",
    "            server = Server(train_dataset, max_contribution_ratio=max_contribution_per_worker)\n",
    "            workers = []\n",
    "            num_byzantine = int((byzantine_percentage / 100) * num_workers)\n",
    "\n",
    "            for i in range(num_workers):\n",
    "                processing_time = random.randint(2, 4)\n",
    "                availability = generate_dynamic_availability(total_time_steps, i, num_workers, max_periods=5, min_processing_time=processing_time)\n",
    "\n",
    "                if i < num_byzantine:\n",
    "                    workers.append(ByzantineWorker(server.model, server.device, i, availability, processing_time))\n",
    "                else:\n",
    "                    workers.append(Worker(server.model, server.device, i, availability, processing_time))\n",
    "\n",
    "            # Lancer la simulation\n",
    "            simulator = DiscreteEventSimulator(server, workers, test_loader, total_time_steps=total_time_steps)\n",
    "            simulator.run()\n",
    "\n",
    "            # Stocker l'accuracy finale\n",
    "            final_accuracy = server.evaluate(test_loader)\n",
    "            accuracies.append(final_accuracy)\n",
    "\n",
    "        # Calcul de la moyenne et stockage\n",
    "        mean_acc = np.mean(accuracies)\n",
    "        mean_accuracies.append(mean_acc)\n",
    "\n",
    "    results[max_contribution_per_worker] = mean_accuracies\n",
    "\n",
    "# üìà Tracer le graphique avec les moyennes pour chaque max_contribution_per_worker\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for max_contribution_per_worker, accuracies in results.items():\n",
    "    plt.plot(byzantine_percentages, accuracies, marker='o', linestyle='-', label=f\"Max Contribution: {max_contribution_per_worker:.1f}\")\n",
    "\n",
    "plt.xlabel(\"Pourcentage de Byzantine Workers\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Impact des Byzantine Workers sur la Pr√©cision du Mod√®le\")\n",
    "plt.legend(title=\"Max Contribution per Worker\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Sauvegarde du graphique\n",
    "plt.savefig(\"impact_byzantine_workers.pdf\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
